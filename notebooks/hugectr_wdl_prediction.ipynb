{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6958f0",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hugectr-wdl-prediction/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Wide and Deep Model with Criteo\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we provide a tutorial that shows how to train a wide and deep model using the high-level Python API from HugeCTR on the original Criteo dataset as training data.\n",
    "We show how to produce prediction results based on different types of local database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235745a8",
   "metadata": {},
   "source": [
    "## Setup HugeCTR\n",
    "\n",
    "To setup the environment, refer to [HugeCTR Example Notebooks](../notebooks) and follow the instructions there before running the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf41447",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "### Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae158c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/wdl_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d463e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "!ls -l $train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7db3c",
   "metadata": {},
   "source": [
    "### Download the original Criteo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da41ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P $train_path https://storage.googleapis.com/criteo-cail-datasets/day_0.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a7cc4-858c-4389-b1c3-586b9b090855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4508791b",
   "metadata": {},
   "source": [
    "\n",
    "Split the dataset into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9620ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 45840617 day_0 > $train_path/train.txt\n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919817c",
   "metadata": {},
   "source": [
    "### Preprocessing with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616c6f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /wdl_train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_train/preprocess.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, LambdaOp, Normalize, Rename, Operator, get_embedding_sizes\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, 'train/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, 'val/temp-parquet-after-conversion')\n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "            shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [(train_input,PREPROCESS_DIR_temp_train),(val_input,PREPROCESS_DIR_temp_val)]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(input,sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output,header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    "\n",
    "    categorify_op = Categorify(freq_threshold=args.freq_limit)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    cross_cat_op = Categorify(encode_type=\"combo\", freq_threshold=args.freq_limit)\n",
    "\n",
    "    features = LABEL_COLUMNS\n",
    "    \n",
    "    if args.criteo_mode == 0:\n",
    "        features += cont_features\n",
    "        if args.feature_cross_list:\n",
    "            feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "            for pair in feature_pairs:\n",
    "                features += [pair] >> cross_cat_op\n",
    "    features += cat_features\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "    logging.info(\"Preprocessing\")\n",
    "\n",
    "    output_format = 'hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format = 'parquet'\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(train_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "    valid_ds_iterator = nvt.Dataset(valid_paths, engine='parquet', part_size=int(args.part_mem_frac * device_size))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "    logging.info('Train Datasets Preprocessing.....')\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    if not args.criteo_mode:\n",
    "        for col in CONTINUOUS_COLUMNS:\n",
    "            dict_dtypes[col] = np.float32\n",
    "    for col in CROSS_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    conts = CONTINUOUS_COLUMNS if not args.criteo_mode else []\n",
    "    \n",
    "    workflow.fit(train_ds_iterator)\n",
    "    \n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(train_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=train_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(train_ds_iterator).to_parquet(\n",
    "                output_path=train_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ###Getting slot size###    \n",
    "    #--------------------##\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    logging.info('Valid Datasets Preprocessing.....')\n",
    "\n",
    "    if output_format == 'hugectr':\n",
    "        workflow.transform(valid_ds_iterator).to_hugectr(\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                output_path=val_output,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "    else:\n",
    "        workflow.transform(valid_ds_iterator).to_parquet(\n",
    "                output_path=val_output,\n",
    "                dtypes=dict_dtypes,\n",
    "                cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                conts=conts,\n",
    "                labels=LABEL_COLUMNS,\n",
    "                shuffle=shuffle,\n",
    "                out_files_per_proc=args.out_files_per_proc,\n",
    "                num_threads=args.num_io_threads)\n",
    "\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS] + [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS]\n",
    "    \n",
    "    print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 1)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--criteo_mode', type=int, default=0)\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7e259b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-10 10:32:34,808 NVTabular processing\n",
      "2023-02-10 10:32:36,590 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-02-10 10:32:36,590 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-02-10 10:32:36,604 Unable to start CUDA Context\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py\", line 782, in _nvmlGetFunctionPointer\n",
      "    _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name)\n",
      "  File \"/usr/lib/python3.8/ctypes/__init__.py\", line 386, in __getattr__\n",
      "    func = self.__getitem__(name)\n",
      "  File \"/usr/lib/python3.8/ctypes/__init__.py\", line 391, in __getitem__\n",
      "    func = self._FuncPtr((name_or_ordinal, self))\n",
      "AttributeError: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetComputeRunningProcesses_v2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/dask_cuda/initialize.py\", line 41, in _create_cuda_context\n",
      "    ctx = has_cuda_context()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/distributed/diagnostics/nvml.py\", line 164, in has_cuda_context\n",
      "    running_processes = pynvml.nvmlDeviceGetComputeRunningProcesses_v2(handle)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py\", line 2191, in nvmlDeviceGetComputeRunningProcesses_v2\n",
      "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeRunningProcesses_v2\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py\", line 785, in _nvmlGetFunctionPointer\n",
      "    raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)\n",
      "pynvml.nvml.NVMLError_FunctionNotFound: Function Not Found\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/core/utils.py:384: FutureWarning: The `client` argument is deprecated from DaskExecutor and will be removed in a future version of NVTabular. By default, a global client in the same python context will be detected automatically, and `merlin.utils.set_dask_client` (as well as `Distributed` and `Serial`) can be used for explicit control.\n",
      "  warnings.warn(\n",
      "2023-02-10 10:32:54,260 Preprocessing\n",
      "2023-02-10 10:32:54,521 Train Datasets Preprocessing.....\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 281564, 415262]\n",
      "2023-02-10 10:34:09,155 Valid Datasets Preprocessing.....\n",
      "[249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 281564, 415262]\n",
      "2023-02-10 10:34:10,596 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /wdl_train/\n",
      "output_path        | /wdl_train/\n",
      "partition size     | 3.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 1\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 92.90126419067383\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 /wdl_train/preprocess.py --data_path /wdl_train/ \\\n",
    "--out_path /wdl_train/ --freq_limit 6 --feature_cross_list C1_C2,C3_C4 \\\n",
    "--device_pool_frac 0.5  --devices '0' --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125c1c9",
   "metadata": {},
   "source": [
    "#### Check the preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c15e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14449264\n",
      "-rw-r--r-- 1 root root          34 Feb 10 10:34 _file_list.txt\n",
      "-rw-r--r-- 1 root root      450893 Feb 10 10:34 _metadata\n",
      "-rw-r--r-- 1 root root        1510 Feb 10 10:34 _metadata.json\n",
      "-rw-r--r-- 1 root root  3245838178 Feb 10 10:34 part_0.parquet\n",
      "-rw-r--r-- 1 root root       27296 Feb 10 10:33 schema.pbtxt\n",
      "drwxr-xr-x 2 root root        4096 Feb 10 10:32 temp-parquet-after-conversion\n",
      "-rw-r--r-- 1 root root 11549710546 Feb 10 10:32 train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -ll /wdl_train/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f671766",
   "metadata": {},
   "source": [
    "### WDL Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad42841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "#from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 4000,\n",
    "                              batchsize_eval = 2720,\n",
    "                              batchsize = 2720,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[2]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"/wdl_train/train/_file_list.txt\"],\n",
    "                                  eval_source = \"/wdl_train/val/_file_list.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0000001)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 1, True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 24,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 405,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=2))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"reshape2\"],\n",
    "                            top_names = [\"wide_redn\"],\n",
    "                            axis = 1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"wide_redn\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"wdl\")\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d1f362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 4.2\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][10:35:38.637][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][10:35:38.637][INFO][RK0][main]: Global seed is 1886280762\n",
      "[HCTR][10:35:38.640][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 2 ->  node 0\n",
      "[HCTR][10:35:40.821][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][10:35:40.821][DEBUG][RK0][main]: [device 2] allocating 0.0000 GB, available 30.8035 \n",
      "[HCTR][10:35:40.821][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][10:35:40.821][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][10:35:40.822][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][10:35:40.823][INFO][RK0][main]: Device 2: Tesla V100-SXM2-32GB\n",
      "[HCTR][10:35:40.824][INFO][RK0][main]: num of DataReader workers for train: 1\n",
      "[HCTR][10:35:40.824][INFO][RK0][main]: num of DataReader workers for eval: 1\n",
      "[HCTR][10:35:40.824][DEBUG][RK0][main]: [device 2] allocating 0.0054 GB, available 30.5476 \n",
      "[HCTR][10:35:40.825][DEBUG][RK0][main]: [device 2] allocating 0.0054 GB, available 30.5417 \n",
      "[HCTR][10:35:40.825][DEBUG][RK0][main]: [device 2] allocating 0.0000 GB, available 30.5417 \n",
      "[HCTR][10:35:40.826][DEBUG][RK0][main]: [device 2] allocating 0.0000 GB, available 30.5417 \n",
      "[HCTR][10:35:40.826][INFO][RK0][main]: Vocabulary size: 2138588\n",
      "[HCTR][10:35:40.826][INFO][RK0][main]: max_vocabulary_size_per_gpu_=2097152\n",
      "[HCTR][10:35:40.838][DEBUG][RK0][main]: [device 2] allocating 0.0241 GB, available 30.3914 \n",
      "[HCTR][10:35:40.845][INFO][RK0][main]: max_vocabulary_size_per_gpu_=2211840\n",
      "[HCTR][10:35:40.851][DEBUG][RK0][main]: [device 2] allocating 0.4288 GB, available 29.9617 \n",
      "[HCTR][10:35:40.851][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][10:35:40.856][DEBUG][RK0][main]: [device 2] allocating 0.2162 GB, available 29.4792 \n",
      "[HCTR][10:35:40.856][DEBUG][RK0][main]: [device 2] allocating 0.0056 GB, available 29.4734 \n",
      "[HCTR][10:35:50.016][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][10:35:50.016][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][10:35:50.016][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][10:35:50.017][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][10:35:50.017][DEBUG][RK0][main]: [device 2] allocating 0.0001 GB, available 29.4734 \n",
      "[HCTR][10:35:50.019][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][10:35:50.023][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][10:35:50.023][INFO][RK0][main]: Model structure on each GPU\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(2720,1)                                (2720,13)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (2720,2,1)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (2720,26,16)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (2720,416)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (2720,2)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReduceSum                               reshape2                      wide_redn                     (2720,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (2720,429)                    \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (2720,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (2720,1)                      \n",
      "                                        wide_redn                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Use non-epoch mode with number of iterations: 21000\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Training source file: /wdl_train/train/_file_list.txt\n",
      "[HCTR][10:35:50.024][INFO][RK0][main]: Evaluation source file: /wdl_train/val/_file_list.txt\n",
      "[HCTR][10:35:55.354][INFO][RK0][main]: Iter: 1000 Time(1000 iters): 5.32919s Loss: 0.117964 lr:0.001\n",
      "[HCTR][10:36:00.665][INFO][RK0][main]: Iter: 2000 Time(1000 iters): 5.30907s Loss: 0.126084 lr:0.001\n",
      "[HCTR][10:36:06.002][INFO][RK0][main]: Iter: 3000 Time(1000 iters): 5.33581s Loss: 0.138335 lr:0.001\n",
      "[HCTR][10:36:11.349][INFO][RK0][main]: Iter: 4000 Time(1000 iters): 5.34525s Loss: 0.101962 lr:0.001\n",
      "[HCTR][10:36:15.933][INFO][RK0][main]: Evaluation, AUC: 0.763947\n",
      "[HCTR][10:36:15.933][INFO][RK0][main]: Eval Time for 4000 iters: 4.583s\n",
      "[HCTR][10:36:21.258][INFO][RK0][main]: Iter: 5000 Time(1000 iters): 9.90761s Loss: 0.120185 lr:0.001\n",
      "[HCTR][10:36:26.620][INFO][RK0][main]: Iter: 6000 Time(1000 iters): 5.35972s Loss: 0.128626 lr:0.001\n",
      "[HCTR][10:36:31.947][INFO][RK0][main]: Iter: 7000 Time(1000 iters): 5.32628s Loss: 0.125264 lr:0.001\n",
      "[HCTR][10:36:37.320][INFO][RK0][main]: Iter: 8000 Time(1000 iters): 5.37131s Loss: 0.121486 lr:0.001\n",
      "[HCTR][10:36:41.803][INFO][RK0][main]: Evaluation, AUC: 0.767916\n",
      "[HCTR][10:36:41.803][INFO][RK0][main]: Eval Time for 4000 iters: 4.48175s\n",
      "[HCTR][10:36:47.154][INFO][RK0][main]: Iter: 9000 Time(1000 iters): 9.83223s Loss: 0.109454 lr:0.001\n",
      "[HCTR][10:36:52.522][INFO][RK0][main]: Iter: 10000 Time(1000 iters): 5.36677s Loss: 0.149472 lr:0.001\n",
      "[HCTR][10:36:57.896][INFO][RK0][main]: Iter: 11000 Time(1000 iters): 5.37183s Loss: 0.118341 lr:0.001\n",
      "[HCTR][10:37:03.264][INFO][RK0][main]: Iter: 12000 Time(1000 iters): 5.36706s Loss: 0.128496 lr:0.001\n",
      "[HCTR][10:37:07.728][INFO][RK0][main]: Evaluation, AUC: 0.769081\n",
      "[HCTR][10:37:07.728][INFO][RK0][main]: Eval Time for 4000 iters: 4.46314s\n",
      "[HCTR][10:37:13.098][INFO][RK0][main]: Iter: 13000 Time(1000 iters): 9.83154s Loss: 0.118482 lr:0.001\n",
      "[HCTR][10:37:18.447][INFO][RK0][main]: Iter: 14000 Time(1000 iters): 5.34802s Loss: 0.122699 lr:0.001\n",
      "[HCTR][10:37:23.812][INFO][RK0][main]: Iter: 15000 Time(1000 iters): 5.36294s Loss: 0.118947 lr:0.001\n",
      "[HCTR][10:37:29.176][INFO][RK0][main]: Iter: 16000 Time(1000 iters): 5.36303s Loss: 0.112516 lr:0.001\n",
      "[HCTR][10:37:33.646][INFO][RK0][main]: Evaluation, AUC: 0.772322\n",
      "[HCTR][10:37:33.646][INFO][RK0][main]: Eval Time for 4000 iters: 4.46896s\n",
      "[HCTR][10:37:39.146][INFO][RK0][main]: Iter: 17000 Time(1000 iters): 9.96806s Loss: 0.11619 lr:0.001\n",
      "[HCTR][10:37:44.517][INFO][RK0][main]: Iter: 18000 Time(1000 iters): 5.37011s Loss: 0.113035 lr:0.001\n",
      "[HCTR][10:37:49.891][INFO][RK0][main]: Iter: 19000 Time(1000 iters): 5.37157s Loss: 0.116589 lr:0.001\n",
      "[HCTR][10:37:55.236][INFO][RK0][main]: Iter: 20000 Time(1000 iters): 5.34424s Loss: 0.127488 lr:0.001\n",
      "[HCTR][10:37:59.698][INFO][RK0][main]: Evaluation, AUC: 0.768523\n",
      "[HCTR][10:37:59.698][INFO][RK0][main]: Eval Time for 4000 iters: 4.46044s\n",
      "[HCTR][10:37:59.698][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:37:59.771][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][10:37:59.810][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:37:59.867][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][10:38:00.091][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][10:38:00.092][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][10:38:00.092][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:38:00.115][INFO][RK0][main]: Done\n",
      "[HCTR][10:38:00.116][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][10:38:00.116][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:38:00.140][INFO][RK0][main]: Done\n",
      "[HCTR][10:38:00.245][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][10:38:00.245][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:38:00.610][INFO][RK0][main]: Done\n",
      "[HCTR][10:38:00.695][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][10:38:00.695][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:38:01.057][INFO][RK0][main]: Done\n",
      "[HCTR][10:38:01.063][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][10:38:01.064][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:38:01.079][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][10:38:01.081][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:38:01.116][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][10:38:06.487][INFO][RK0][main]: Finish 21000 iterations with batchsize: 2720 in 136.46s.\n",
      "[HCTR][10:38:06.488][INFO][RK0][main]: Save the model graph to wdl.json successfully\n",
      "[1676025486.656545] [dgx1v-loki-23:602  :0] cuda_copy_iface.c:468  UCX  ERROR cuCtxGetCurrent(&cuda_context)() failed: p�\u0001\n",
      "[1676025486.656590] [dgx1v-loki-23:602  :0]  cuda_ipc_iface.c:531  UCX  ERROR cuCtxGetCurrent(&cuda_context)() failed: \n",
      "[HCTR][10:38:06.707][INFO][RK0][main]: MPI finalization done.\n"
     ]
    }
   ],
   "source": [
    "!python ./model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649a6a9",
   "metadata": {},
   "source": [
    "## Prepare Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "130dd6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 633080\n",
      "-rw-r--r-- 1 root root        32 Feb 10 10:34 _file_list.txt\n",
      "-rw-r--r-- 1 root root     21894 Feb 10 10:34 _metadata\n",
      "-rw-r--r-- 1 root root      1509 Feb 10 10:34 _metadata.json\n",
      "-rw-r--r-- 1 root root 138441430 Feb 10 10:34 part_0.parquet\n",
      "-rw-r--r-- 1 root root     27296 Feb 10 10:34 schema.pbtxt\n",
      "drwxr-xr-x 2 root root        50 Feb 10 10:32 temp-parquet-after-conversion\n",
      "-rw-r--r-- 1 root root 509766965 Feb 10 10:32 test.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l /wdl_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449cb610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.054112</td>\n",
       "      <td>-0.267624</td>\n",
       "      <td>0.371471</td>\n",
       "      <td>-0.076760</td>\n",
       "      <td>-0.131771</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.208772</td>\n",
       "      <td>0.324461</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>402</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.048792</td>\n",
       "      <td>-0.547466</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>0.949396</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1645</td>\n",
       "      <td>0</td>\n",
       "      <td>1358</td>\n",
       "      <td>1232</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059432</td>\n",
       "      <td>-0.516221</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.115539</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>-0.687732</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33190</td>\n",
       "      <td>32473</td>\n",
       "      <td>34242</td>\n",
       "      <td>0</td>\n",
       "      <td>954</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.029284</td>\n",
       "      <td>-0.548824</td>\n",
       "      <td>-0.057773</td>\n",
       "      <td>-0.105099</td>\n",
       "      <td>-0.224758</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.255725</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>622</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.061206</td>\n",
       "      <td>-0.548824</td>\n",
       "      <td>-0.594327</td>\n",
       "      <td>-0.145369</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>0.339399</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>143</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.054112 -0.267624  0.371471 -0.076760 -0.131771 -0.206385 -0.064249   \n",
       "1 -0.048792 -0.547466 -0.594327 -0.157301 -0.224758 -0.206385 -0.064249   \n",
       "2 -0.059432 -0.516221 -0.594327 -0.115539 -0.209261 -0.206385 -0.064249   \n",
       "3 -0.029284 -0.548824 -0.057773 -0.105099 -0.224758 -0.206385 -0.064249   \n",
       "4 -0.061206 -0.548824 -0.594327 -0.145369 -0.209261 -0.206385  0.339399   \n",
       "\n",
       "         I8        I9       I10  ...  C17  C18  C19    C20    C21    C22  \\\n",
       "0 -0.208772  0.324461 -0.470383  ...    1   49    1      3      3      3   \n",
       "1  0.949396 -0.760031 -0.470383  ...    3    2    1      0   1645      0   \n",
       "2 -0.281810 -0.687732 -0.470383  ...    0    1    1  33190  32473  34242   \n",
       "3 -0.255725 -0.760031 -0.470383  ...    1    1    2      1      1      1   \n",
       "4 -0.281810  0.035263 -0.470383  ...    0    1    1     90    143    101   \n",
       "\n",
       "    C23   C24  C25  C26  \n",
       "0     9   402    1    4  \n",
       "1  1358  1232    1    1  \n",
       "2     0   954    3    3  \n",
       "3     1   622    1    2  \n",
       "4     0    21    1    3  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/val/part_0.parquet\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85069108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).to_csv('/wdl_train/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f219ba",
   "metadata": {},
   "source": [
    "## Create prediction scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e98d68e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/wdl_train/wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, enable_cache, use_rocksdb=False, rocksdb_path=None):\n",
    "    CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]+[\"C1_C2\",\"C3_C4\"]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    emb_size = [249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "    \n",
    "    \n",
    "    persistent_db_params = hugectr.inference.PersistentDatabaseParams()\n",
    "    if use_rocksdb:\n",
    "        persistent_db_params = hugectr.inference.PersistentDatabaseParams(\n",
    "                                  backend = hugectr.DatabaseType_t.rocks_db,\n",
    "                                  path = rocksdb_path\n",
    "                                )\n",
    "    \n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                persistent_db = persistent_db_params,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    print(\"{} multi-embedding table prediction\".format(model_name))\n",
    "    network_file = sys.argv[2]\n",
    "    print(\"{} multi-embedding table prediction network is {}\".format(model_name,network_file))\n",
    "    dense_file = sys.argv[3]\n",
    "    print(\"{} multi-embedding table prediction dense file is {}\".format(model_name,dense_file))\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(\"{} multi-embedding table prediction sparse files are {}\".format(model_name,embedding_file_list))\n",
    "    data_file = sys.argv[5]\n",
    "    print(\"{} multi-embedding table prediction input data path is {}\".format(model_name,data_file))\n",
    "    input_dbtype = sys.argv[6]\n",
    "    print(\"{} multi-embedding table prediction input dbtype path is {}\".format(model_name,input_dbtype))\n",
    "    if input_dbtype==\"disabled\":\n",
    "        wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    if input_dbtype==\"rocksdb\":\n",
    "        rocksdb_path = sys.argv[7]\n",
    "        print(\"{} multi-embedding table prediction rocksdb_path path is {}\".format(model_name,rocksdb_path))\n",
    "        wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True, True, rocksdb_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bdd0f",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Use different types of databases as a local parameter server to get the wide and deep model prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b1f54",
   "metadata": {},
   "source": [
    "### Load model embedding tables into local memory as parameter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "604cb6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdl multi-embedding table prediction\n",
      "wdl multi-embedding table prediction network is ./wdl.json\n",
      "wdl multi-embedding table prediction dense file is ./wdl_dense_20000.model\n",
      "wdl multi-embedding table prediction sparse files are ['./wdl0_sparse_20000.model/', './wdl1_sparse_20000.model']\n",
      "wdl multi-embedding table prediction input data path is /wdl_train/infer_test.csv\n",
      "wdl multi-embedding table prediction input dbtype path is disabled\n",
      "[HCTR][10:53:44.990][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][10:53:44.990][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][10:53:44.990][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][10:53:44.990][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][10:53:44.991][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][10:53:44.991][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][10:53:44.991][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][10:53:44.991][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][10:53:44.991][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:53:45.479][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding2; cached 664320 / 664320 embeddings in volatile database (HashMapBackend); load: 664320 / 18446744073709551615 (0.00%).\n",
      "[HCTR][10:53:45.479][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:53:45.829][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding1; cached 1030499 / 1030499 embeddings in volatile database (HashMapBackend); load: 1030499 / 18446744073709551615 (0.00%).\n",
      "[HCTR][10:53:45.836][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][10:53:45.836][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Max batch size: 64\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Use static table: False\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: Configured cache hit rate threshold: 0.500000\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: The size of worker memory pool: 2\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][10:53:45.842][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][10:53:46.786][INFO][RK0][main]: Global seed is 1984285016\n",
      "[HCTR][10:53:46.789][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][10:53:47.797][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][10:53:47.797][DEBUG][RK0][main]: [device 0] allocating 0.0000 GB, available 30.7156 \n",
      "[HCTR][10:53:47.797][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][10:53:47.797][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: Max batchsize: 64\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: sparse_input name wide_data\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: sparse_input name deep_data\n",
      "[HCTR][10:53:47.798][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][10:53:47.798][DEBUG][RK0][main]: [device 0] allocating 0.0003 GB, available 30.4636 \n",
      "[HCTR][10:53:47.799][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HCTR][10:53:47.799][DEBUG][RK0][main]: [device 0] allocating 0.0128 GB, available 30.4421 \n",
      "WDL multi-embedding table inference result is [0.011136045679450035, 0.006747737061232328, 0.005509266164153814, 0.0118627417832613, 0.01798960007727146, 0.010030664503574371, 0.02108118124306202, 0.008684462867677212, 0.07753805071115494, 0.011398322880268097]\n"
     ]
    }
   ],
   "source": [
    "!python /wdl_train/wdl_predict.py \"wdl\" \"./wdl.json\" \"./wdl_dense_20000.model\" \"./wdl0_sparse_20000.model/,./wdl1_sparse_20000.model\" \"/wdl_train/infer_test.csv\" \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284260e",
   "metadata": {},
   "source": [
    "### Load model embedding tables into local RocksDB as a parameter Server\n",
    "\n",
    "Create a RocksDB directory with read and write permissions for storing model embedded tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9f750d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p -m 700 /wdl_train/rocksdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c59e4b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdl multi-embedding table prediction\n",
      "wdl multi-embedding table prediction network is ./wdl.json\n",
      "wdl multi-embedding table prediction dense file is ./wdl_dense_20000.model\n",
      "wdl multi-embedding table prediction sparse files are ['./wdl0_sparse_20000.model/', './wdl1_sparse_20000.model']\n",
      "wdl multi-embedding table prediction input data path is /wdl_train/infer_test.csv\n",
      "wdl multi-embedding table prediction input dbtype path is rocksdb\n",
      "wdl multi-embedding table prediction rocksdb_path path is /wdl_train/rocksdb\n",
      "[HCTR][10:56:41.546][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][10:56:41.546][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][10:56:41.546][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][10:56:41.546][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][10:56:41.547][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][10:56:41.547][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][10:56:41.547][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][10:56:41.547][INFO][RK0][main]: Creating RocksDB backend...\n",
      "[HCTR][10:56:41.547][INFO][RK0][main]: Connecting to RocksDB database...\n",
      "[HCTR][10:56:41.548][ERROR][RK0][main]: RocksDB /wdl_train/rocksdb: Listing column names failed!\n",
      "[HCTR][10:56:41.548][INFO][RK0][main]: RocksDB /wdl_train/rocksdb, found column family \"default\".\n",
      "[HCTR][10:56:41.583][INFO][RK0][main]: Connected to RocksDB database!\n",
      "[HCTR][10:56:41.583][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][10:56:41.583][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:56:42.084][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding2; cached 664320 / 664320 embeddings in volatile database (HashMapBackend); load: 664320 / 18446744073709551615 (0.00%).\n",
      "[HCTR][10:56:43.351][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding2; cached 664320 embeddings in persistent database (RocksDB).\n",
      "[HCTR][10:56:43.351][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][10:56:43.693][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding1; cached 1030499 / 1030499 embeddings in volatile database (HashMapBackend); load: 1030499 / 18446744073709551615 (0.00%).\n",
      "[HCTR][10:56:45.979][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding1; cached 1030499 embeddings in persistent database (RocksDB).\n",
      "[HCTR][10:56:45.985][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][10:56:45.985][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Max batch size: 64\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Use static table: False\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: Configured cache hit rate threshold: 0.500000\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: The size of worker memory pool: 2\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][10:56:45.991][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][10:56:46.953][INFO][RK0][main]: Global seed is 3196997041\n",
      "[HCTR][10:56:46.956][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][10:56:48.005][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][10:56:48.005][DEBUG][RK0][main]: [device 0] allocating 0.0000 GB, available 30.7156 \n",
      "[HCTR][10:56:48.005][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][10:56:48.005][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: Max batchsize: 64\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: sparse_input name wide_data\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: sparse_input name deep_data\n",
      "[HCTR][10:56:48.006][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][10:56:48.006][DEBUG][RK0][main]: [device 0] allocating 0.0003 GB, available 30.4636 \n",
      "[HCTR][10:56:48.007][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HCTR][10:56:48.007][DEBUG][RK0][main]: [device 0] allocating 0.0128 GB, available 30.4421 \n",
      "WDL multi-embedding table inference result is [0.011136045679450035, 0.006747737061232328, 0.005509266164153814, 0.0118627417832613, 0.01798960007727146, 0.010030664503574371, 0.02108118124306202, 0.008684462867677212, 0.07753805071115494, 0.011398322880268097]\n",
      "[HCTR][10:56:48.571][INFO][RK0][main]: Disconnecting from RocksDB database...\n",
      "[HCTR][10:56:48.573][INFO][RK0][main]: Disconnected from RocksDB database!\n"
     ]
    }
   ],
   "source": [
    "!python /wdl_train/wdl_predict.py \"wdl\" \"./wdl.json\" \\\n",
    "\"./wdl_dense_20000.model\" \\\n",
    "\"./wdl0_sparse_20000.model/,./wdl1_sparse_20000.model\" \\\n",
    "\"/wdl_train/infer_test.csv\" \\\n",
    "\"rocksdb\"  \"/wdl_train/rocksdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4d2db-3098-463f-a895-01479cf2e93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
