{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Continuous Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "The notebook introduces how to use the model oversubscription (a.k.a. Embedding Training Cache) feature in HugeCTR for the continuous training. The Model Oversubscription feature is designed to handle recommendation models with huge embedding table by means of incremental training method, which allows you to train such a model even the model size is much larger than the available GPU memory size.\n",
    "\n",
    "Model oversubscription currently supports the following features:\n",
    "- Support single-node multi-GPUs and multi-node multi-GPUs cases\n",
    "- Support both the SSD-based parameter server (PS) and the host memory (HMEM)-based PS\n",
    "  * For the SSD-based PS, the embedding table size can scale up to the SSD capacity\n",
    "  * For the HMEM-based PS, the embedding table size can scale up to the aggregate HMEM size of each node\n",
    "- Provide the `get_incremental_model()` interface to get the updated embedding features during training\n",
    "- Support training from scratch or incremental training with existing sparse and dense models\n",
    "\n",
    "To learn more about the model oversubscription, please check the [Embedding Training Cache](../docs/hugectr_user_guide.md#embedding-training-cache).\n",
    "\n",
    "To learn how to use the APIs of model oversubscription, please check the [HugeCTR Python Interface](../docs/python_interface.md).\n",
    "\n",
    "## Table of Contents\n",
    "-  [Installation](#1)\n",
    "   * [Get HugeCTR from NGC](#11)\n",
    "   * [Build HugeCTR from Source Code](#12)\n",
    "-  [Continuous Training](#2)\n",
    "   * [Data Preparation](#21)\n",
    "   * [Continuous Training with High-level API](#22)\n",
    "   * [Continuous Training with Low-level API](#23)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1\"></a>\n",
    "## Installation\n",
    "\n",
    "### 1.1 Get HugeCTR from NGC\n",
    "The continuous training module is preinstalled in the [Merlin Training Container](https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-training): `nvcr.io/nvidia/merlin/merlin-training:0.7`.\n",
    "\n",
    "You can check the existence of required libraries by running the following Python code after launching this container.\n",
    "```bash\n",
    "$ python3 -c \"import hugectr\"\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Build HugeCTR from Source Code\n",
    "\n",
    "If you want to build HugeCTR from the source code instead of using the NGC container, please refer to the [Setup development environment](../docs/hugectr_contributor_guide.md#setup-development-environment)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2\"></a>\n",
    "## Continuous Training\n",
    "\n",
    "<a id=\"21\"></a>\n",
    "### 2.1 Data Preparation\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ cd ${project_root}/tools\n",
    "   $ wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz\n",
    "   ```\n",
    "   \n",
    "   To preprocess the downloaded Kaggle Criteo dataset, we'll make the following operations: \n",
    "   * Reduce the amounts of data to speed up the preprocessing\n",
    "   * Fill missing values\n",
    "   * Remove the feature values whose occurrences are very rare, etc.\n",
    "\n",
    "2. Preprocessing by Pandas using the following command:\n",
    "   ```shell\n",
    "   $ bash preprocess.sh 1 wdl_data pandas 1 1 100\n",
    "   ```\n",
    "   \n",
    "   Meanings of the command line arguments:\n",
    "   * The 1st argument represents the dataset postfix. It is `1` here since `day_1` is used.\n",
    "   * The 2nd argument `wdl_data` is where the preprocessed data is stored.\n",
    "   * The 3rd argument `pandas` is the processing script going to use, here we choose `pandas`.\n",
    "   * The 4th argument `1` embodies that the normalization is applied to dense features.\n",
    "   * The 5th argument `1` means that the feature crossing is applied.\n",
    "   * The 6th argument `100` means the number of data files in each file list.\n",
    "\n",
    "   For more details about the data preprocessing, please refer to [Preprocess the Criteo Dataset](../samples/criteo#preprocess-the-dataset).\n",
    "   \n",
    "3. Create a soft link of the dataset folder to the path of this notebook using the following command:\n",
    "   ```shell\n",
    "   $ ln ${project_root}/tools/wdl_data ${project_root}/notebooks/wdl_data\n",
    "   ```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"22\"></a>\n",
    "### 2.2 Continuous Training with High-level API\n",
    "This section gives the code sample of continuous training using Keras-like high-level API. The high-level API encapsulates most of the complexity for users, making it easy to use and able to handle most of scenarios in a production environment.\n",
    "\n",
    "Meanwhile, HugeCTR also provides the low-level APIs besides its high-level counterpart to allow you customize the training logic. A code sample using the low-level APIs is provided in the next section.\n",
    "\n",
    "The code sample in this section trains a model from scratch using model oversubscriber, gets the incremental model, and saves the trained dense weights and sparse embedding weights. The following steps are required to achieve those logics:\n",
    "\n",
    "1. Create the `solver`, `reader`, `optimizer` and `mos`, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding, and dense layers in order.\n",
    "3. Compile the model and overview the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Train the sparse and dense model.\n",
    "6. Set the new training datasets and their corresponding keysets.\n",
    "7. Train the sparse and dense model incrementally.\n",
    "8. Get the incrementally trained embedding table.\n",
    "9. Save the model weights and optimizer states explicitly.\n",
    "\n",
    "Note: `repeat_dataset` should be `False` when using the model oversubscriber, while the argument `num_epochs` in `Model::fit` specifies the number of training epochs in this mode."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%writefile wdl_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"wdl_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"wdl_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "mos = hugectr.CreateMOS(train_from_scratch = True, use_host_memory_ps = True, dest_sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"])\n",
    "model = hugectr.Model(solver, reader, optimizer, mos)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 30, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "model.set_source(source = [\"wdl_data/file_list.3.txt\", \"wdl_data/file_list.4.txt\"], keyset = [\"wdl_data/file_list.3.keyset\", \"wdl_data/file_list.4.keyset\"], eval_source = \"wdl_data/file_list.5.txt\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "updated_model = model.get_incremental_model()\n",
    "model.save_params_to_files(\"wdl_mos\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting wdl_train.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!python3 wdl_train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[06d03h17m42s][HUGECTR][INFO]: Global seed is 3804219231\n",
      "[06d03h17m43s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[06d03h17m44s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[06d03h17m44s][HUGECTR][INFO]: Start all2all warmup\n",
      "[06d03h17m44s][HUGECTR][INFO]: End all2all warmup\n",
      "[06d03h17m44s][HUGECTR][INFO]: Using All-reduce algorithm NCCL\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "[06d03h17m44s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[06d03h17m45s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[06d03h17m45s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "===================================================Model Compile===================================================\n",
      "[06d03h17m48s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[06d03h17m48s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[06d03h17m48s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[06d03h17m48s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[06d03h17m48s][HUGECTR][INFO]: Host MEM-based Parameter Server is enabled\n",
      "[06d03h17m48s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_0_sparse_model\n",
      "[06d03h17m48s][HUGECTR][INFO]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[06d03h17m48s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_1_sparse_model\n",
      "[06d03h17m48s][HUGECTR][INFO]: ./wdl_1_sparse_model not exist, create and train from scratch\n",
      "[06d03h17m49s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[06d03h17m49s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "Add                                     fc3,reshape2                  add1                          (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add1,label                    loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[60d30h17m49s][HUGECTR][INFO]: Save the model graph to wdl.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[60d30h17m49s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 1\n",
      "[60d30h17m49s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[60d30h17m49s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[60d30h17m49s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[60d30h17m49s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[60d30h17m49s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[60d30h17m49s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.2.txt\n",
      "[60d30h17m49s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.0.txt--------------------\n",
      "[60d30h17m49s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h17m53s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 3.872256s Loss: 0.134935 lr:0.001000\n",
      "[60d30h17m56s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.053273s Loss: 0.138988 lr:0.001000\n",
      "[60d30h18m00s][HUGECTR][INFO]: Evaluation, AUC: 0.738853\n",
      "[60d30h18m00s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.385942s\n",
      "[60d30h18m30s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 7.407449s Loss: 0.144995 lr:0.001000\n",
      "[60d30h18m60s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.043145s Loss: 0.146232 lr:0.001000\n",
      "[60d30h18m90s][HUGECTR][INFO]: Evaluation, AUC: 0.745530\n",
      "[60d30h18m90s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.324962s\n",
      "[60d30h18m12s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 5.336096s Loss: 0.136061 lr:0.001000\n",
      "[60d30h18m15s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.007950s Loss: 0.124782 lr:0.001000\n",
      "[60d30h18m17s][HUGECTR][INFO]: Evaluation, AUC: 0.751169\n",
      "[60d30h18m17s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.317395s\n",
      "[60d30h18m20s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 5.328225s Loss: 0.118722 lr:0.001000\n",
      "[60d30h18m23s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.002698s Loss: 0.140604 lr:0.001000\n",
      "[60d30h18m25s][HUGECTR][INFO]: Evaluation, AUC: 0.754128\n",
      "[60d30h18m25s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.326376s\n",
      "[60d30h18m25s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.1.txt--------------------\n",
      "[60d30h18m25s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h18m30s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 6.928943s Loss: 0.117650 lr:0.001000\n",
      "[60d30h18m33s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 3.077998s Loss: 0.121734 lr:0.001000\n",
      "[60d30h18m35s][HUGECTR][INFO]: Evaluation, AUC: 0.755767\n",
      "[60d30h18m35s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.318333s\n",
      "[60d30h18m38s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 5.391155s Loss: 0.138971 lr:0.001000\n",
      "[60d30h18m42s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 3.390838s Loss: 0.098762 lr:0.001000\n",
      "[60d30h18m44s][HUGECTR][INFO]: Evaluation, AUC: 0.755616\n",
      "[60d30h18m44s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.343156s\n",
      "[60d30h18m47s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 5.432850s Loss: 0.161411 lr:0.001000\n",
      "[60d30h18m50s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 3.112100s Loss: 0.150486 lr:0.001000\n",
      "[60d30h18m53s][HUGECTR][INFO]: Evaluation, AUC: 0.757646\n",
      "[60d30h18m53s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.325587s\n",
      "[60d30h18m56s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 5.647138s Loss: 0.124576 lr:0.001000\n",
      "[60d30h18m59s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 3.006313s Loss: 0.127446 lr:0.001000\n",
      "[60d30h19m10s][HUGECTR][INFO]: Evaluation, AUC: 0.760428\n",
      "[60d30h19m10s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.314648s\n",
      "=====================================================Model Fit=====================================================\n",
      "[60d30h19m10s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 1\n",
      "[60d30h19m10s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[60d30h19m10s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[60d30h19m10s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[60d30h19m10s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[60d30h19m10s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[60d30h19m10s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.5.txt\n",
      "[60d30h19m10s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.3.txt--------------------\n",
      "[60d30h19m10s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h19m50s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 3.871890s Loss: 0.121446 lr:0.001000\n",
      "[60d30h19m80s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.007659s Loss: 0.110337 lr:0.001000\n",
      "[60d30h19m11s][HUGECTR][INFO]: Evaluation, AUC: 0.759322\n",
      "[60d30h19m11s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.329360s\n",
      "[60d30h19m14s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 5.440972s Loss: 0.125984 lr:0.001000\n",
      "[60d30h19m17s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.243298s Loss: 0.132992 lr:0.001000\n",
      "[60d30h19m23s][HUGECTR][INFO]: Evaluation, AUC: 0.758439\n",
      "[60d30h19m23s][HUGECTR][INFO]: Eval Time for 5000 iters: 5.904707s\n",
      "[60d30h19m26s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 8.916865s Loss: 0.107345 lr:0.001000\n",
      "[60d30h19m29s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.009296s Loss: 0.129040 lr:0.001000\n",
      "[60d30h19m31s][HUGECTR][INFO]: Evaluation, AUC: 0.760264\n",
      "[60d30h19m31s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.328651s\n",
      "[60d30h19m34s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 5.340272s Loss: 0.134015 lr:0.001000\n",
      "[60d30h19m37s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.001674s Loss: 0.155767 lr:0.001000\n",
      "[60d30h19m39s][HUGECTR][INFO]: Evaluation, AUC: 0.762740\n",
      "[60d30h19m39s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.311858s\n",
      "[60d30h19m39s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.4.txt--------------------\n",
      "[60d30h19m40s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h19m44s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 6.868067s Loss: 0.149758 lr:0.001000\n",
      "[60d30h19m47s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 3.022998s Loss: 0.139501 lr:0.001000\n",
      "[60d30h19m49s][HUGECTR][INFO]: Evaluation, AUC: 0.761255\n",
      "[60d30h19m49s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.346880s\n",
      "[60d30h19m52s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 5.361537s Loss: 0.128456 lr:0.001000\n",
      "[60d30h19m55s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 3.033604s Loss: 0.130820 lr:0.001000\n",
      "[60d30h19m58s][HUGECTR][INFO]: Evaluation, AUC: 0.760711\n",
      "[60d30h19m58s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.331019s\n",
      "[60d30h20m10s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 5.388210s Loss: 0.147991 lr:0.001000\n",
      "[60d30h20m40s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 3.004750s Loss: 0.140950 lr:0.001000\n",
      "[60d30h20m60s][HUGECTR][INFO]: Evaluation, AUC: 0.762486\n",
      "[60d30h20m60s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.317332s\n",
      "[60d30h20m90s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 5.326406s Loss: 0.124543 lr:0.001000\n",
      "[60d30h20m12s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 3.228303s Loss: 0.136586 lr:0.001000\n",
      "[60d30h20m15s][HUGECTR][INFO]: Evaluation, AUC: 0.762719\n",
      "[60d30h20m15s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.313440s\n",
      "[60d30h20m17s][HUGECTR][INFO]: Get updated portion of embedding table [DONE}\n",
      "[60d30h20m17s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[60d30h20m20s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[60d30h20m21s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[60d30h20m21s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[60d30h20m21s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Continuous Training with Low-level API\n",
    "\n",
    "This section gives the code sample of continuous training using low-level API, which follows the same logics as the code sample in above section.\n",
    "\n",
    "Although the low-level APIs provide fine-grind control to the training logic, we encourage you to use the high-level API if it can satisfy your requirement since the naked data reader and model oversubscriber logics are not straightforward and error prone.\n",
    "\n",
    "For more about the low-level API, please refer to [Low-level Training API](../docs/python_interface.md#low-level-training-api) and samples of [Low-level Training](./hugectr_criteo.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%%writefile wdl_mos.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"wdl_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"wdl_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "mos = hugectr.CreateMOS(train_from_scratch = True, use_host_memory_ps = True, dest_sparse_models = [\"./wdl_low_0_sparse_model\", \"./wdl_low_1_sparse_model\"])\n",
    "model = hugectr.Model(solver, reader, optimizer, mos)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "data_reader_train = model.get_data_reader_train()\n",
    "data_reader_eval = model.get_data_reader_eval()\n",
    "model_oversubscriber = model.get_model_oversubscriber()\n",
    "dataset = [(\"wdl_data/file_list.\"+str(i)+\".txt\", \"wdl_data/file_list.\"+str(i)+\".keyset\") for i in range(2)]\n",
    "data_reader_eval.set_source(\"wdl_data/file_list.2.txt\")\n",
    "data_reader_eval_flag = True\n",
    "iteration = 0\n",
    "for file_list, keyset_file in dataset:\n",
    "  data_reader_train.set_source(file_list)\n",
    "  data_reader_train_flag = True\n",
    "  model_oversubscriber.update(keyset_file)\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    data_reader_train_flag = model.train()\n",
    "    if not data_reader_train_flag:\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while data_reader_eval_flag:\n",
    "        if batches >= solver.max_eval_batches:\n",
    "          break\n",
    "        data_reader_eval_flag = model.eval()\n",
    "        batches += 1\n",
    "      if not data_reader_eval_flag:\n",
    "        data_reader_eval.set_source()\n",
    "        data_reader_eval_flag = True\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "  print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "\n",
    "dataset = [(\"wdl_data/file_list.\"+str(i)+\".txt\", \"wdl_data/file_list.\"+str(i)+\".keyset\") for i in range(3, 5)]\n",
    "for file_list, keyset_file in dataset:\n",
    "  data_reader_train.set_source(file_list)\n",
    "  data_reader_train_flag = True\n",
    "  model_oversubscriber.update(keyset_file)\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    data_reader_train_flag = model.train()\n",
    "    if not data_reader_train_flag:\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while data_reader_eval_flag:\n",
    "        if batches >= solver.max_eval_batches:\n",
    "          break\n",
    "        data_reader_eval_flag = model.eval()\n",
    "        batches += 1\n",
    "      if not data_reader_eval_flag:\n",
    "        data_reader_eval.set_source()\n",
    "        data_reader_eval_flag = True\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "  print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "incremental_model = model.get_incremental_model()\n",
    "model.save_params_to_files(\"wdl_mos\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting wdl_mos.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!python3 wdl_train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[06d03h23m52s][HUGECTR][INFO]: Global seed is 463704757\n",
      "[06d03h23m53s][HUGECTR][INFO]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[06d03h23m55s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[06d03h23m55s][HUGECTR][INFO]: Start all2all warmup\n",
      "[06d03h23m55s][HUGECTR][INFO]: End all2all warmup\n",
      "[06d03h23m55s][HUGECTR][INFO]: Using All-reduce algorithm NCCL\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "[06d03h23m55s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[06d03h23m55s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[06d03h23m55s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "===================================================Model Compile===================================================\n",
      "[06d03h23m59s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[06d03h23m59s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[06d03h23m59s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[06d03h23m59s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[06d03h23m59s][HUGECTR][INFO]: Host MEM-based Parameter Server is enabled\n",
      "[06d03h23m59s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_0_sparse_model\n",
      "[06d03h23m59s][HUGECTR][INFO]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[06d03h23m59s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_1_sparse_model\n",
      "[06d03h23m59s][HUGECTR][INFO]: ./wdl_1_sparse_model not exist, create and train from scratch\n",
      "[06d03h23m59s][HUGECTR][INFO]: Starting AUC NCCL warm-up\n",
      "[06d03h23m59s][HUGECTR][INFO]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "Add                                     fc3,reshape2                  add1                          (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add1,label                    loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[60d30h23m59s][HUGECTR][INFO]: Save the model graph to wdl.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[60d30h23m59s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 1\n",
      "[60d30h23m59s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[60d30h23m59s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[60d30h23m59s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[60d30h23m59s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[60d30h23m59s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[60d30h23m59s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.2.txt\n",
      "[60d30h23m59s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.0.txt--------------------\n",
      "[60d30h23m59s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h24m20s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 3.231428s Loss: 0.135521 lr:0.001000\n",
      "[60d30h24m50s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.079515s Loss: 0.139057 lr:0.001000\n",
      "[60d30h24m80s][HUGECTR][INFO]: Evaluation, AUC: 0.740329\n",
      "[60d30h24m80s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.320645s\n",
      "[60d30h24m11s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 5.404498s Loss: 0.145556 lr:0.001000\n",
      "[60d30h24m14s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.080483s Loss: 0.148771 lr:0.001000\n",
      "[60d30h24m16s][HUGECTR][INFO]: Evaluation, AUC: 0.746889\n",
      "[60d30h24m16s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.324233s\n",
      "[60d30h24m19s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 5.406876s Loss: 0.137966 lr:0.001000\n",
      "[60d30h24m22s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.077814s Loss: 0.123059 lr:0.001000\n",
      "[60d30h24m25s][HUGECTR][INFO]: Evaluation, AUC: 0.753738\n",
      "[60d30h24m25s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.320030s\n",
      "[60d30h24m28s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 5.399150s Loss: 0.118834 lr:0.001000\n",
      "[60d30h24m31s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.070449s Loss: 0.143509 lr:0.001000\n",
      "[60d30h24m33s][HUGECTR][INFO]: Evaluation, AUC: 0.756078\n",
      "[60d30h24m33s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.327226s\n",
      "[60d30h24m33s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.1.txt--------------------\n",
      "[60d30h24m33s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h24m38s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 6.803557s Loss: 0.117378 lr:0.001000\n",
      "[60d30h24m41s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 3.075633s Loss: 0.119654 lr:0.001000\n",
      "[60d30h24m43s][HUGECTR][INFO]: Evaluation, AUC: 0.757430\n",
      "[60d30h24m43s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.306454s\n",
      "[60d30h24m46s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 5.393352s Loss: 0.139031 lr:0.001000\n",
      "[60d30h24m49s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 3.078417s Loss: 0.097426 lr:0.001000\n",
      "[60d30h24m52s][HUGECTR][INFO]: Evaluation, AUC: 0.758502\n",
      "[60d30h24m52s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.312437s\n",
      "[60d30h24m55s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 5.393159s Loss: 0.160444 lr:0.001000\n",
      "[60d30h24m58s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 3.075678s Loss: 0.148892 lr:0.001000\n",
      "[60d30h25m00s][HUGECTR][INFO]: Evaluation, AUC: 0.759402\n",
      "[60d30h25m00s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.339000s\n",
      "[60d30h25m30s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 5.420728s Loss: 0.123998 lr:0.001000\n",
      "[60d30h25m60s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 3.076093s Loss: 0.126488 lr:0.001000\n",
      "[60d30h25m80s][HUGECTR][INFO]: Evaluation, AUC: 0.761669\n",
      "[60d30h25m80s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.317491s\n",
      "=====================================================Model Fit=====================================================\n",
      "[60d30h25m80s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 1\n",
      "[60d30h25m80s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[60d30h25m90s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[60d30h25m90s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[60d30h25m90s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[60d30h25m90s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[60d30h25m90s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.5.txt\n",
      "[60d30h25m90s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.3.txt--------------------\n",
      "[60d30h25m90s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h25m13s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 4.014837s Loss: 0.119070 lr:0.001000\n",
      "[60d30h25m16s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.076699s Loss: 0.109741 lr:0.001000\n",
      "[60d30h25m18s][HUGECTR][INFO]: Evaluation, AUC: 0.761593\n",
      "[60d30h25m18s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.311787s\n",
      "[60d30h25m21s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 5.392611s Loss: 0.125033 lr:0.001000\n",
      "[60d30h25m24s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.076238s Loss: 0.135769 lr:0.001000\n",
      "[60d30h25m26s][HUGECTR][INFO]: Evaluation, AUC: 0.759262\n",
      "[60d30h25m26s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.320817s\n",
      "[60d30h25m29s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 5.402925s Loss: 0.106197 lr:0.001000\n",
      "[60d30h25m33s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.078704s Loss: 0.123226 lr:0.001000\n",
      "[60d30h25m35s][HUGECTR][INFO]: Evaluation, AUC: 0.761417\n",
      "[60d30h25m35s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.314426s\n",
      "[60d30h25m38s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 5.397134s Loss: 0.135866 lr:0.001000\n",
      "[60d30h25m41s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.070462s Loss: 0.156328 lr:0.001000\n",
      "[60d30h25m43s][HUGECTR][INFO]: Evaluation, AUC: 0.763851\n",
      "[60d30h25m43s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.311082s\n",
      "[60d30h25m43s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.4.txt--------------------\n",
      "[60d30h25m43s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[60d30h25m48s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 6.569502s Loss: 0.148958 lr:0.001000\n",
      "[60d30h25m51s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 3.078324s Loss: 0.135236 lr:0.001000\n",
      "[60d30h25m53s][HUGECTR][INFO]: Evaluation, AUC: 0.762436\n",
      "[60d30h25m53s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.315148s\n",
      "[60d30h25m56s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 5.399809s Loss: 0.131039 lr:0.001000\n",
      "[60d30h25m59s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 3.077266s Loss: 0.132457 lr:0.001000\n",
      "[60d30h26m20s][HUGECTR][INFO]: Evaluation, AUC: 0.762696\n",
      "[60d30h26m20s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.319179s\n",
      "[60d30h26m50s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 5.400596s Loss: 0.150583 lr:0.001000\n",
      "[60d30h26m80s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 3.078670s Loss: 0.136845 lr:0.001000\n",
      "[60d30h26m10s][HUGECTR][INFO]: Evaluation, AUC: 0.762776\n",
      "[60d30h26m10s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.330047s\n",
      "[60d30h26m13s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 5.409189s Loss: 0.127056 lr:0.001000\n",
      "[60d30h26m16s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 3.075967s Loss: 0.135671 lr:0.001000\n",
      "[60d30h26m18s][HUGECTR][INFO]: Evaluation, AUC: 0.763311\n",
      "[60d30h26m18s][HUGECTR][INFO]: Eval Time for 5000 iters: 2.322623s\n",
      "[60d30h26m20s][HUGECTR][INFO]: Get updated portion of embedding table [DONE}\n",
      "[60d30h26m20s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[60d30h26m23s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[60d30h26m24s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[60d30h26m24s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[60d30h26m24s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}