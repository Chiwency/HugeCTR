{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4337359",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-sok-to-dlrm-demo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HPS Inference DLRM with SOK weights Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac179f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to enable HPS to use SOK's embedding weights for inference. \n",
    "\n",
    "Note, before running this notebook, you must first run the [sok_train_hps_inference_demo.ipynb](sok_train_hps_inference_demo.ipynb) notebook to generate SOK's weights, otherwise, this notebook will crash!\n",
    "\n",
    "For more details about SOK, please refer to [SOK Documentation](https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html). For more details about HPS APIs, please refer to [HPS APIs](https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/api/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202109ad",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get SOK from NGC\n",
    "\n",
    "SOK and HPS Python modules are preinstalled in the 23.12 and later [Merlin Tensorflow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:nightly`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import sparse_operation_kit as sok\"\n",
    "$ python3 -c \"import hierarchical_parameter_server as hps\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c936f7",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "First, we need to convert the SOK trained key-value weights into a form that HPS can use as key-value weights. The function generate_kv_file_for_hps is the weight conversion function. Additionally, to run HPS, some configuration information is also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039e7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate hps weight success!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '4'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "import json\n",
    "\n",
    "args = dict()\n",
    "args[\"gpu_num\"] = 1 \n",
    "args[\"slot_num\"] = 26                             # the number of feature fields in this embedding layer\n",
    "args[\"iter_num\"] = 10                             # the number of feature fields in this embedding layer\n",
    "args[\"embed_vec_sizes\"] = [16]*args[\"slot_num\"]                       # the dimension of embedding vectors\n",
    "args[\"dense_dim\"] = 13                            # the dimension of dense features\n",
    "args[\"global_batch_size\"] = 1024                 # the globally batchsize for all GPUs\n",
    "args[\"table_names\"] = [\"table\"+str(i) for i in range(args[\"slot_num\"])]                            # embedding table names\n",
    "args[\"max_vocabulary_sizes\"] = np.random.randint(1000, 1200, size=args[\"slot_num\"]).tolist()\n",
    "args[\"max_nnz\"] = [1]*args[\"slot_num\"]\n",
    "\n",
    "args[\"ps_config_file\"] = \"dlrm.json\"\n",
    "args[\"dense_model_path\"] = \"dlrm_dense.model\"\n",
    "args[\"sparse_model_path\"] = \"dlrm_sparse.model\"\n",
    "args[\"saved_path\"] = \"dlrm_tf_saved_model\"\n",
    "args[\"np_key_type\"] = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"] = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "args[\"sok_embedding_table_path\"] = \"sok_dlrm_sparse.model\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))\n",
    "\n",
    "data = {\n",
    "    \"supportlonglong\": True,\n",
    "    \"models\": [{\n",
    "        \"model\": \"dlrm\",\n",
    "        \"sparse_files\": [\"dlrm_sparse.model/table\"+str(i) for i in range(args[\"slot_num\"])],\n",
    "        \"num_of_worker_buffer_in_pool\": 30,\n",
    "        \"embedding_table_names\": [\"table\"+str(i) for i in range(args[\"slot_num\"])],\n",
    "        \"embedding_vecsize_per_table\": [16]*args[\"slot_num\"],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [10]*args[\"slot_num\"],\n",
    "        \"default_value_for_each_table\": [1.0]*args[\"slot_num\"],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 1024,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": True\n",
    "    }]\n",
    "}\n",
    "\n",
    "\n",
    "def generate_kv_file_for_hps(args):\n",
    "\n",
    "    def convert_sok_weights_for_hps(sok_file_path,dtype,dim,output_path):\n",
    "       file_head_length=296\n",
    "       with open(sok_file_path, \"rb\") as f:\n",
    "           f.seek(file_head_length, os.SEEK_SET)\n",
    "           array_np = np.fromfile(f, dtype=dtype)\n",
    "           array_np = array_np.reshape((-1,dim))\n",
    "       with open(output_path, mode=\"wb\") as f:\n",
    "           array_np.tofile(f)\n",
    "\n",
    "    sok_weight_file = args[\"sok_embedding_table_path\"]+\"/\"\n",
    "    hps_spase_model_path =  args[\"sparse_model_path\"]+\"/\"\n",
    "    if not os.path.exists(hps_spase_model_path):\n",
    "        os.makedirs(hps_spase_model_path, exist_ok=True)\n",
    "    table_names = args[\"table_names\"]\n",
    "    for i,table_name in enumerate(table_names):\n",
    "        table_output_path = hps_spase_model_path+table_name+\"/\"\n",
    "        if not os.path.exists(table_output_path):\n",
    "            os.makedirs(table_output_path, exist_ok=True)\n",
    "\n",
    "        #Note:The suffix \"_0\" is an automatic numbering added by SOK to prevent users from inputting duplicate table names.\n",
    "        #For example, if the names of two sok.DynamicVariable are \"table1\", the first created will have the name \"table1_0\" \n",
    "        #and the subsequently created will have the name \"table1_1\". \n",
    "        #In this example, there are no duplicate names inputted, so SOK generates the weight names as name+\"_0\".\n",
    "        key_filename = table_name+\"_0-key\" \n",
    "        value_filename = table_name+\"_0-weight\"\n",
    "        key_path = sok_weight_file+key_filename\n",
    "        value_path = sok_weight_file+value_filename\n",
    "        \n",
    "        key_output_path= table_output_path + \"key\"\n",
    "        value_output_path= table_output_path + \"emb_vector\"\n",
    "        convert_sok_weights_for_hps(key_path,args[\"np_key_type\"],1,key_output_path)\n",
    "        convert_sok_weights_for_hps(value_path,args[\"np_vector_type\"],args[\"embed_vec_sizes\"][i],value_output_path)\n",
    "    print(\"generate hps weight success!\")\n",
    "    \n",
    "generate_kv_file_for_hps(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4cac2",
   "metadata": {},
   "source": [
    "Define how to generate dataset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c74eff6-6a27-4b23-a3eb-9559d0685ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] hierarchical_parameter_server is imported\n"
     ]
    }
   ],
   "source": [
    "with open('dlrm.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "import hierarchical_parameter_server as hps\n",
    "\n",
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, max_nnz, dense_dim):\n",
    "    def generate_sparse_keys(num_samples, vocabulary_range_per_slot, max_nnz, key_dtype = args[\"np_key_type\"]):\n",
    "        slot_num = len(vocabulary_range_per_slot)\n",
    "        total_indices = []\n",
    "        for i in range(slot_num):\n",
    "            indices = []\n",
    "            values = []\n",
    "            for j in range(num_samples):\n",
    "                vocab_range = vocabulary_range_per_slot[i]\n",
    "                nnz = np.random.randint(low=1, high=max_nnz+1)\n",
    "                entries = sorted(np.random.choice(max_nnz, nnz, replace=False))\n",
    "                for entry in entries:\n",
    "                    indices.append([j, 0, entry])\n",
    "                values.extend(np.random.randint(low=0, high=vocab_range, size=(nnz, )))\n",
    "            values = np.array(values, dtype=key_dtype)\n",
    "            total_indices.append(tf.sparse.SparseTensor(indices = indices,\n",
    "                                    values = values,\n",
    "                                    dense_shape = (num_samples, 1, max_nnz)))\n",
    "        return total_indices\n",
    "\n",
    "    sparse_keys = generate_sparse_keys(num_samples, vocabulary_range_per_slot, max_nnz)\n",
    "    dense_features = np.random.random((num_samples, dense_dim)).astype(np.float32)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return sparse_keys, dense_features, labels\n",
    "\n",
    "def tf_dataset(sparse_keys, dense_features, labels, batchsize):\n",
    "    total_data = []\n",
    "    #total_data.extend(sparse_keys)\n",
    "    total_data.extend(sparse_keys)\n",
    "    total_data.append(dense_features)\n",
    "    total_data.append(labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tuple(total_data))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74fa65",
   "metadata": {},
   "source": [
    "## Build model with HPS embedding layers\n",
    "\n",
    "Create an inference model using `hps.SparseLookupLayer` and args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f685fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 26, 16)]     0           []                               \n",
      "                                                                                                  \n",
      " bottom (MLP)                   (None, 16)           38544       ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " second_order_feature_interacti  (None, None)        0           ['input_28[0][0]']               \n",
      " on (SecondOrderFeatureInteract                                                                   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " dense_concat2 (Concatenate)    (None, None)         0           ['bottom[0][0]',                 \n",
      "                                                                  'second_order_feature_interactio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " top (MLP)                      (None, 1)            120577      ['dense_concat2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 159,121\n",
      "Trainable params: 159,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " sparse_lookup_layer (SparseLoo  (None, 16)          0           ['input_1[0][0]']                \n",
      " kupLayer)                                                                                        \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_1 (SparseL  (None, 16)          0           ['input_2[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_2 (SparseL  (None, 16)          0           ['input_3[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_3 (SparseL  (None, 16)          0           ['input_4[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_4 (SparseL  (None, 16)          0           ['input_5[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_5 (SparseL  (None, 16)          0           ['input_6[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_6 (SparseL  (None, 16)          0           ['input_7[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_7 (SparseL  (None, 16)          0           ['input_8[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_8 (SparseL  (None, 16)          0           ['input_9[0][0]']                \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_9 (SparseL  (None, 16)          0           ['input_10[0][0]']               \n",
      " ookupLayer)                                                                                      \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_10 (Sparse  (None, 16)          0           ['input_11[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_11 (Sparse  (None, 16)          0           ['input_12[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_12 (Sparse  (None, 16)          0           ['input_13[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_13 (Sparse  (None, 16)          0           ['input_14[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_14 (Sparse  (None, 16)          0           ['input_15[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_15 (Sparse  (None, 16)          0           ['input_16[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_16 (Sparse  (None, 16)          0           ['input_17[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_17 (Sparse  (None, 16)          0           ['input_18[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_18 (Sparse  (None, 16)          0           ['input_19[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_19 (Sparse  (None, 16)          0           ['input_20[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_20 (Sparse  (None, 16)          0           ['input_21[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_21 (Sparse  (None, 16)          0           ['input_22[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_22 (Sparse  (None, 16)          0           ['input_23[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_23 (Sparse  (None, 16)          0           ['input_24[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_24 (Sparse  (None, 16)          0           ['input_25[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " sparse_lookup_layer_25 (Sparse  (None, 16)          0           ['input_26[0][0]']               \n",
      " LookupLayer)                                                                                     \n",
      "                                                                                                  \n",
      " reshape0 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer[0][0]']    \n",
      "                                                                                                  \n",
      " reshape1 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_1[0][0]']  \n",
      "                                                                                                  \n",
      " reshape2 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_2[0][0]']  \n",
      "                                                                                                  \n",
      " reshape3 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_3[0][0]']  \n",
      "                                                                                                  \n",
      " reshape4 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_4[0][0]']  \n",
      "                                                                                                  \n",
      " reshape5 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_5[0][0]']  \n",
      "                                                                                                  \n",
      " reshape6 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_6[0][0]']  \n",
      "                                                                                                  \n",
      " reshape7 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_7[0][0]']  \n",
      "                                                                                                  \n",
      " reshape8 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_8[0][0]']  \n",
      "                                                                                                  \n",
      " reshape9 (Reshape)             (None, 1, 16)        0           ['sparse_lookup_layer_9[0][0]']  \n",
      "                                                                                                  \n",
      " reshape10 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_10[0][0]'] \n",
      "                                                                                                  \n",
      " reshape11 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_11[0][0]'] \n",
      "                                                                                                  \n",
      " reshape12 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_12[0][0]'] \n",
      "                                                                                                  \n",
      " reshape13 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_13[0][0]'] \n",
      "                                                                                                  \n",
      " reshape14 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_14[0][0]'] \n",
      "                                                                                                  \n",
      " reshape15 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_15[0][0]'] \n",
      "                                                                                                  \n",
      " reshape16 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_16[0][0]'] \n",
      "                                                                                                  \n",
      " reshape17 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_17[0][0]'] \n",
      "                                                                                                  \n",
      " reshape18 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_18[0][0]'] \n",
      "                                                                                                  \n",
      " reshape19 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_19[0][0]'] \n",
      "                                                                                                  \n",
      " reshape20 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_20[0][0]'] \n",
      "                                                                                                  \n",
      " reshape21 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_21[0][0]'] \n",
      "                                                                                                  \n",
      " reshape22 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_22[0][0]'] \n",
      "                                                                                                  \n",
      " reshape23 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_23[0][0]'] \n",
      "                                                                                                  \n",
      " reshape24 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_24[0][0]'] \n",
      "                                                                                                  \n",
      " reshape25 (Reshape)            (None, 1, 16)        0           ['sparse_lookup_layer_25[0][0]'] \n",
      "                                                                                                  \n",
      " concat1 (Concatenate)          (None, 26, 16)       0           ['reshape0[0][0]',               \n",
      "                                                                  'reshape1[0][0]',               \n",
      "                                                                  'reshape2[0][0]',               \n",
      "                                                                  'reshape3[0][0]',               \n",
      "                                                                  'reshape4[0][0]',               \n",
      "                                                                  'reshape5[0][0]',               \n",
      "                                                                  'reshape6[0][0]',               \n",
      "                                                                  'reshape7[0][0]',               \n",
      "                                                                  'reshape8[0][0]',               \n",
      "                                                                  'reshape9[0][0]',               \n",
      "                                                                  'reshape10[0][0]',              \n",
      "                                                                  'reshape11[0][0]',              \n",
      "                                                                  'reshape12[0][0]',              \n",
      "                                                                  'reshape13[0][0]',              \n",
      "                                                                  'reshape14[0][0]',              \n",
      "                                                                  'reshape15[0][0]',              \n",
      "                                                                  'reshape16[0][0]',              \n",
      "                                                                  'reshape17[0][0]',              \n",
      "                                                                  'reshape18[0][0]',              \n",
      "                                                                  'reshape19[0][0]',              \n",
      "                                                                  'reshape20[0][0]',              \n",
      "                                                                  'reshape21[0][0]',              \n",
      "                                                                  'reshape22[0][0]',              \n",
      "                                                                  'reshape23[0][0]',              \n",
      "                                                                  'reshape24[0][0]',              \n",
      "                                                                  'reshape25[0][0]']              \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 1)            159121      ['concat1[0][0]',                \n",
      "                                                                  'input_27[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 159,121\n",
      "Trainable params: 159,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_78 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as bottom_0_layer_call_fn, bottom_0_layer_call_and_return_conditional_losses, bottom_1_layer_call_fn, bottom_1_layer_call_and_return_conditional_losses, bottom_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dlrm_tf_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dlrm_tf_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "class InferenceModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_model_path,\n",
    "                 **kwargs):\n",
    "        super(InferenceModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        \n",
    "        self.sparse_lookup_layer_list = []\n",
    "        for i in range(self.slot_num):\n",
    "            self.sparse_lookup_layer_list.append(hps.SparseLookupLayer(model_name = \"dlrm\", \n",
    "                                                table_id = i,\n",
    "                                                emb_vec_size = self.embed_vec_size[i],\n",
    "                                                emb_vec_dtype = args[\"tf_vector_type\"]))\n",
    "        self.reshape_layer_list = []\n",
    "        for i in range(self.slot_num):\n",
    "            self.reshape_layer_list.append(tf.keras.layers.Reshape((1, args[\"embed_vec_sizes\"][i]), name = \"reshape\"+str(i)))\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=1, name = \"concat1\")\n",
    "\n",
    "        self.dense_model = tf.keras.models.load_model(dense_model_path)\n",
    "        self.dense_model.summary()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        #input_sparse = inputs[:]\n",
    "        input_sparse = inputs[:-1]\n",
    "        input_dense = inputs[-1]\n",
    "        embeddings = []\n",
    "        for i in range(self.slot_num):\n",
    "            tmp_embedding = self.sparse_lookup_layer_list[i](sp_ids=input_sparse[i], sp_weights=None, combiner=\"mean\")\n",
    "            embeddings.append(tmp_embedding)\n",
    "\n",
    "        concat_embeddings = []\n",
    "        for i in range(args[\"slot_num\"]):\n",
    "            concat_embeddings.append(self.reshape_layer_list[i](embeddings[i]))\n",
    "        concat_embeddings = self.concat1(concat_embeddings)\n",
    "\n",
    "        logit = self.dense_model([concat_embeddings,input_dense])\n",
    "        return logit, embeddings\n",
    "    def summary(self):\n",
    "        inputs = []\n",
    "        for i in range(args[\"slot_num\"]):\n",
    "            inputs.append(tf.keras.Input(shape=(args[\"max_nnz\"][i], ), sparse=True, dtype=args[\"tf_key_type\"])) \n",
    "        dense_input = tf.keras.Input(shape=(args[\"dense_dim\"], ), dtype=tf.float32)\n",
    "        inputs.append(dense_input)\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()\n",
    "    \n",
    "def create_and_save_inference_graph(args): \n",
    "    model = InferenceModel(args[\"slot_num\"], args[\"embed_vec_sizes\"], args[\"dense_model_path\"])\n",
    "    model.summary()\n",
    "    inputs = []\n",
    "    for i in range(args[\"slot_num\"]):\n",
    "        inputs.append(tf.keras.Input(shape=(args[\"max_nnz\"][i], ), sparse=True, dtype=args[\"tf_key_type\"])) \n",
    "    dense_input = tf.keras.Input(shape=(args[\"dense_dim\"], ), dtype=tf.float32)\n",
    "    inputs.append(dense_input)\n",
    "    _ = model(inputs)\n",
    "    model.save(args[\"saved_path\"])\n",
    "\n",
    "create_and_save_inference_graph(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79673a06-0db7-4a41-bfe9-20e7160a3f6b",
   "metadata": {},
   "source": [
    "## Inference with HPS models\n",
    "\n",
    "Run the HPS model and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b516ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: fuse_embedding_table is not specified using default: 0\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][04:03:31.190][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: use_static_table is not specified using default: 0\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: use_context_stream is not specified using default: 1\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: use_hctr_cache_implementation is not specified using default: 1\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: thread_pool_size is not specified using default: 16\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: init_ec is not specified using default: 1\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: enable_pagelock is not specified using default: 0\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: fp8_quant is not specified using default: 0\n",
      "[HCTR][04:03:31.190][INFO][RK0][main]: HPS plugin uses context stream for model dlrm: True\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][04:03:31.191][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][04:03:31.191][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][04:03:31.191][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][04:03:31.191][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][04:03:31.191][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][04:03:31.538][INFO][RK0][main]: Table: hps_et.dlrm.table0; cached 1184 / 1184 embeddings in volatile database (HashMapBackend); load: 1105 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:31.895][INFO][RK0][main]: Table: hps_et.dlrm.table1; cached 1186 / 1186 embeddings in volatile database (HashMapBackend); load: 1108 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:32.267][INFO][RK0][main]: Table: hps_et.dlrm.table2; cached 1146 / 1146 embeddings in volatile database (HashMapBackend); load: 1141 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:32.665][INFO][RK0][main]: Table: hps_et.dlrm.table3; cached 1023 / 1023 embeddings in volatile database (HashMapBackend); load: 1023 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:33.063][INFO][RK0][main]: Table: hps_et.dlrm.table4; cached 1170 / 1170 embeddings in volatile database (HashMapBackend); load: 1069 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:33.458][INFO][RK0][main]: Table: hps_et.dlrm.table5; cached 1065 / 1065 embeddings in volatile database (HashMapBackend); load: 1004 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:33.864][INFO][RK0][main]: Table: hps_et.dlrm.table6; cached 1084 / 1084 embeddings in volatile database (HashMapBackend); load: 1034 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:34.242][INFO][RK0][main]: Table: hps_et.dlrm.table7; cached 1095 / 1095 embeddings in volatile database (HashMapBackend); load: 1054 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:34.630][INFO][RK0][main]: Table: hps_et.dlrm.table8; cached 1101 / 1101 embeddings in volatile database (HashMapBackend); load: 1064 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:35.018][INFO][RK0][main]: Table: hps_et.dlrm.table9; cached 1006 / 1006 embeddings in volatile database (HashMapBackend); load: 994 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:35.406][INFO][RK0][main]: Table: hps_et.dlrm.table10; cached 1195 / 1195 embeddings in volatile database (HashMapBackend); load: 1125 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:35.813][INFO][RK0][main]: Table: hps_et.dlrm.table11; cached 1041 / 1041 embeddings in volatile database (HashMapBackend); load: 958 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:36.222][INFO][RK0][main]: Table: hps_et.dlrm.table12; cached 1143 / 1143 embeddings in volatile database (HashMapBackend); load: 1136 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:36.628][INFO][RK0][main]: Table: hps_et.dlrm.table13; cached 1189 / 1189 embeddings in volatile database (HashMapBackend); load: 1112 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:37.008][INFO][RK0][main]: Table: hps_et.dlrm.table14; cached 1181 / 1181 embeddings in volatile database (HashMapBackend); load: 1097 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:37.388][INFO][RK0][main]: Table: hps_et.dlrm.table15; cached 1068 / 1068 embeddings in volatile database (HashMapBackend); load: 1010 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:37.783][INFO][RK0][main]: Table: hps_et.dlrm.table16; cached 1106 / 1106 embeddings in volatile database (HashMapBackend); load: 1073 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:38.190][INFO][RK0][main]: Table: hps_et.dlrm.table17; cached 1147 / 1147 embeddings in volatile database (HashMapBackend); load: 1143 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:38.565][INFO][RK0][main]: Table: hps_et.dlrm.table18; cached 1014 / 1014 embeddings in volatile database (HashMapBackend); load: 1006 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:38.945][INFO][RK0][main]: Table: hps_et.dlrm.table19; cached 1057 / 1057 embeddings in volatile database (HashMapBackend); load: 992 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:39.320][INFO][RK0][main]: Table: hps_et.dlrm.table20; cached 1072 / 1072 embeddings in volatile database (HashMapBackend); load: 1016 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:39.695][INFO][RK0][main]: Table: hps_et.dlrm.table21; cached 1167 / 1167 embeddings in volatile database (HashMapBackend); load: 1064 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:40.067][INFO][RK0][main]: Table: hps_et.dlrm.table22; cached 1064 / 1064 embeddings in volatile database (HashMapBackend); load: 1001 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:40.441][INFO][RK0][main]: Table: hps_et.dlrm.table23; cached 1124 / 1124 embeddings in volatile database (HashMapBackend); load: 1102 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:40.815][INFO][RK0][main]: Table: hps_et.dlrm.table24; cached 1107 / 1107 embeddings in volatile database (HashMapBackend); load: 1074 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:41.199][INFO][RK0][main]: Table: hps_et.dlrm.table25; cached 1147 / 1147 embeddings in volatile database (HashMapBackend); load: 1143 / 18446744073709551615 (0.00%).\n",
      "[HCTR][04:03:41.205][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][04:03:41.205][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Model name: dlrm\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Max batch size: 1024\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Fuse embedding tables: False\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Number of embedding tables: 26\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Embedding cache type: dynamic\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: The size of thread pool: 32\n",
      "[HCTR][04:03:41.207][INFO][RK0][main]: The size of worker memory pool: 30\n",
      "[HCTR][04:03:41.208][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][04:03:41.208][INFO][RK0][main]: The refresh percentage : 0.200000\n",
      "[HCTR][04:03:41.642][INFO][RK0][main]: Initialize the embedding cache by by inserting the same size model file with embedding cache from beginning\n",
      "[HCTR][04:03:41.643][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][04:03:41.643][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table0\n",
      "[HCTR][04:03:41.647][INFO][RK0][main]: Initialize the embedding table 0 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.650][INFO][RK0][main]: Initialize the embedding table 0 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.654][INFO][RK0][main]: Initialize the embedding table 0 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.658][INFO][RK0][main]: Initialize the embedding table 0 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.661][INFO][RK0][main]: Initialize the embedding table 0 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.665][INFO][RK0][main]: Initialize the embedding table 0 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:41.666][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table1\n",
      "[HCTR][04:03:41.669][INFO][RK0][main]: Initialize the embedding table 1 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.673][INFO][RK0][main]: Initialize the embedding table 1 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.676][INFO][RK0][main]: Initialize the embedding table 1 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.680][INFO][RK0][main]: Initialize the embedding table 1 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.683][INFO][RK0][main]: Initialize the embedding table 1 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.687][INFO][RK0][main]: Initialize the embedding table 1 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:41.687][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table2\n",
      "[HCTR][04:03:41.691][INFO][RK0][main]: Initialize the embedding table 2 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.694][INFO][RK0][main]: Initialize the embedding table 2 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.701][INFO][RK0][main]: Initialize the embedding table 2 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.705][INFO][RK0][main]: Initialize the embedding table 2 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.708][INFO][RK0][main]: Initialize the embedding table 2 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.708][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table3\n",
      "[HCTR][04:03:41.712][INFO][RK0][main]: Initialize the embedding table 3 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.715][INFO][RK0][main]: Initialize the embedding table 3 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.719][INFO][RK0][main]: Initialize the embedding table 3 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.723][INFO][RK0][main]: Initialize the embedding table 3 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.727][INFO][RK0][main]: Initialize the embedding table 3 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.727][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table4\n",
      "[HCTR][04:03:41.730][INFO][RK0][main]: Initialize the embedding table 4 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.734][INFO][RK0][main]: Initialize the embedding table 4 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.738][INFO][RK0][main]: Initialize the embedding table 4 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.742][INFO][RK0][main]: Initialize the embedding table 4 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.745][INFO][RK0][main]: Initialize the embedding table 4 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.749][INFO][RK0][main]: Initialize the embedding table 4 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:41.749][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table5\n",
      "[HCTR][04:03:41.753][INFO][RK0][main]: Initialize the embedding table 5 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.757][INFO][RK0][main]: Initialize the embedding table 5 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.760][INFO][RK0][main]: Initialize the embedding table 5 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.764][INFO][RK0][main]: Initialize the embedding table 5 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.767][INFO][RK0][main]: Initialize the embedding table 5 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.767][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table6\n",
      "[HCTR][04:03:41.771][INFO][RK0][main]: Initialize the embedding table 6 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.774][INFO][RK0][main]: Initialize the embedding table 6 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.778][INFO][RK0][main]: Initialize the embedding table 6 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.782][INFO][RK0][main]: Initialize the embedding table 6 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.786][INFO][RK0][main]: Initialize the embedding table 6 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.786][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table7\n",
      "[HCTR][04:03:41.789][INFO][RK0][main]: Initialize the embedding table 7 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.793][INFO][RK0][main]: Initialize the embedding table 7 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.797][INFO][RK0][main]: Initialize the embedding table 7 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.801][INFO][RK0][main]: Initialize the embedding table 7 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.805][INFO][RK0][main]: Initialize the embedding table 7 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.805][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table8\n",
      "[HCTR][04:03:41.809][INFO][RK0][main]: Initialize the embedding table 8 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.812][INFO][RK0][main]: Initialize the embedding table 8 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.816][INFO][RK0][main]: Initialize the embedding table 8 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.820][INFO][RK0][main]: Initialize the embedding table 8 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.823][INFO][RK0][main]: Initialize the embedding table 8 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.823][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table9\n",
      "[HCTR][04:03:41.827][INFO][RK0][main]: Initialize the embedding table 9 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.830][INFO][RK0][main]: Initialize the embedding table 9 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.834][INFO][RK0][main]: Initialize the embedding table 9 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.838][INFO][RK0][main]: Initialize the embedding table 9 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.841][INFO][RK0][main]: Initialize the embedding table 9 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.841][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table10\n",
      "[HCTR][04:03:41.845][INFO][RK0][main]: Initialize the embedding table 10 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.848][INFO][RK0][main]: Initialize the embedding table 10 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.852][INFO][RK0][main]: Initialize the embedding table 10 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.856][INFO][RK0][main]: Initialize the embedding table 10 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.859][INFO][RK0][main]: Initialize the embedding table 10 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.863][INFO][RK0][main]: Initialize the embedding table 10 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:41.863][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table11\n",
      "[HCTR][04:03:41.867][INFO][RK0][main]: Initialize the embedding table 11 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.870][INFO][RK0][main]: Initialize the embedding table 11 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.874][INFO][RK0][main]: Initialize the embedding table 11 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.877][INFO][RK0][main]: Initialize the embedding table 11 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.881][INFO][RK0][main]: Initialize the embedding table 11 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.881][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table12\n",
      "[HCTR][04:03:41.885][INFO][RK0][main]: Initialize the embedding table 12 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.888][INFO][RK0][main]: Initialize the embedding table 12 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.892][INFO][RK0][main]: Initialize the embedding table 12 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.896][INFO][RK0][main]: Initialize the embedding table 12 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.899][INFO][RK0][main]: Initialize the embedding table 12 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.899][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table13\n",
      "[HCTR][04:03:41.903][INFO][RK0][main]: Initialize the embedding table 13 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.906][INFO][RK0][main]: Initialize the embedding table 13 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.910][INFO][RK0][main]: Initialize the embedding table 13 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.914][INFO][RK0][main]: Initialize the embedding table 13 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.918][INFO][RK0][main]: Initialize the embedding table 13 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.921][INFO][RK0][main]: Initialize the embedding table 13 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:41.921][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table14\n",
      "[HCTR][04:03:41.925][INFO][RK0][main]: Initialize the embedding table 14 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.929][INFO][RK0][main]: Initialize the embedding table 14 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.933][INFO][RK0][main]: Initialize the embedding table 14 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.936][INFO][RK0][main]: Initialize the embedding table 14 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.940][INFO][RK0][main]: Initialize the embedding table 14 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.943][INFO][RK0][main]: Initialize the embedding table 14 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:41.944][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table15\n",
      "[HCTR][04:03:41.947][INFO][RK0][main]: Initialize the embedding table 15 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.951][INFO][RK0][main]: Initialize the embedding table 15 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.954][INFO][RK0][main]: Initialize the embedding table 15 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.958][INFO][RK0][main]: Initialize the embedding table 15 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.962][INFO][RK0][main]: Initialize the embedding table 15 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.962][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table16\n",
      "[HCTR][04:03:41.966][INFO][RK0][main]: Initialize the embedding table 16 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.969][INFO][RK0][main]: Initialize the embedding table 16 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.973][INFO][RK0][main]: Initialize the embedding table 16 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.976][INFO][RK0][main]: Initialize the embedding table 16 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.980][INFO][RK0][main]: Initialize the embedding table 16 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.980][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table17\n",
      "[HCTR][04:03:41.984][INFO][RK0][main]: Initialize the embedding table 17 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:41.987][INFO][RK0][main]: Initialize the embedding table 17 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:41.991][INFO][RK0][main]: Initialize the embedding table 17 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:41.995][INFO][RK0][main]: Initialize the embedding table 17 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:41.998][INFO][RK0][main]: Initialize the embedding table 17 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:41.999][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table18\n",
      "[HCTR][04:03:42.002][INFO][RK0][main]: Initialize the embedding table 18 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.005][INFO][RK0][main]: Initialize the embedding table 18 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.009][INFO][RK0][main]: Initialize the embedding table 18 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.012][INFO][RK0][main]: Initialize the embedding table 18 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.016][INFO][RK0][main]: Initialize the embedding table 18 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.016][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table19\n",
      "[HCTR][04:03:42.019][INFO][RK0][main]: Initialize the embedding table 19 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.023][INFO][RK0][main]: Initialize the embedding table 19 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.027][INFO][RK0][main]: Initialize the embedding table 19 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.030][INFO][RK0][main]: Initialize the embedding table 19 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.034][INFO][RK0][main]: Initialize the embedding table 19 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.034][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table20\n",
      "[HCTR][04:03:42.037][INFO][RK0][main]: Initialize the embedding table 20 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.041][INFO][RK0][main]: Initialize the embedding table 20 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.044][INFO][RK0][main]: Initialize the embedding table 20 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.048][INFO][RK0][main]: Initialize the embedding table 20 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.051][INFO][RK0][main]: Initialize the embedding table 20 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.051][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table21\n",
      "[HCTR][04:03:42.055][INFO][RK0][main]: Initialize the embedding table 21 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.058][INFO][RK0][main]: Initialize the embedding table 21 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.062][INFO][RK0][main]: Initialize the embedding table 21 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.065][INFO][RK0][main]: Initialize the embedding table 21 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.069][INFO][RK0][main]: Initialize the embedding table 21 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.073][INFO][RK0][main]: Initialize the embedding table 21 for iteration 5 with number of 192 keys.\n",
      "[HCTR][04:03:42.073][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table22\n",
      "[HCTR][04:03:42.076][INFO][RK0][main]: Initialize the embedding table 22 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.080][INFO][RK0][main]: Initialize the embedding table 22 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.083][INFO][RK0][main]: Initialize the embedding table 22 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.087][INFO][RK0][main]: Initialize the embedding table 22 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.091][INFO][RK0][main]: Initialize the embedding table 22 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.091][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table23\n",
      "[HCTR][04:03:42.094][INFO][RK0][main]: Initialize the embedding table 23 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.097][INFO][RK0][main]: Initialize the embedding table 23 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.101][INFO][RK0][main]: Initialize the embedding table 23 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.104][INFO][RK0][main]: Initialize the embedding table 23 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.108][INFO][RK0][main]: Initialize the embedding table 23 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.108][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table24\n",
      "[HCTR][04:03:42.111][INFO][RK0][main]: Initialize the embedding table 24 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.115][INFO][RK0][main]: Initialize the embedding table 24 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.118][INFO][RK0][main]: Initialize the embedding table 24 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.122][INFO][RK0][main]: Initialize the embedding table 24 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.125][INFO][RK0][main]: Initialize the embedding table 24 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.125][INFO][RK0][main]: EC initialization on device 0 for hps_et.dlrm.table25\n",
      "[HCTR][04:03:42.129][INFO][RK0][main]: Initialize the embedding table 25 for iteration 0 with number of 192 keys.\n",
      "[HCTR][04:03:42.132][INFO][RK0][main]: Initialize the embedding table 25 for iteration 1 with number of 192 keys.\n",
      "[HCTR][04:03:42.136][INFO][RK0][main]: Initialize the embedding table 25 for iteration 2 with number of 192 keys.\n",
      "[HCTR][04:03:42.139][INFO][RK0][main]: Initialize the embedding table 25 for iteration 3 with number of 192 keys.\n",
      "[HCTR][04:03:42.143][INFO][RK0][main]: Initialize the embedding table 25 for iteration 4 with number of 192 keys.\n",
      "[HCTR][04:03:42.146][INFO][RK0][main]: LookupSession i64_input_key: True\n",
      "[HCTR][04:03:42.146][INFO][RK0][main]: Creating lookup session for dlrm on device: 0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inference_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sparse_lookup_layer (Sparse  multiple                 0         \n",
      " LookupLayer)                                                    \n",
      "                                                                 \n",
      " sparse_lookup_layer_1 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_2 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_3 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_4 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_5 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_6 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_7 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_8 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_9 (Spar  multiple                 0         \n",
      " seLookupLayer)                                                  \n",
      "                                                                 \n",
      " sparse_lookup_layer_10 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_11 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_12 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_13 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_14 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_15 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_16 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_17 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_18 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_19 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_20 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_21 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_22 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_23 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_24 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " sparse_lookup_layer_25 (Spa  multiple                 0         \n",
      " rseLookupLayer)                                                 \n",
      "                                                                 \n",
      " reshape0 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape1 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape2 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape3 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape4 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape5 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape6 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape7 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape8 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape9 (Reshape)          multiple                  0         \n",
      "                                                                 \n",
      " reshape10 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape11 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape12 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape13 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape14 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape15 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape16 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape17 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape18 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape19 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape20 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape21 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape22 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape23 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape24 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " reshape25 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " concat1 (Concatenate)       multiple                  0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 159121    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159,121\n",
      "Trainable params: 159,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-------------------- Step 0 --------------------\n",
      "-------------------- Step 1 --------------------\n",
      "-------------------- Step 2 --------------------\n",
      "-------------------- Step 3 --------------------\n",
      "-------------------- Step 4 --------------------\n",
      "-------------------- Step 5 --------------------\n",
      "-------------------- Step 6 --------------------\n",
      "-------------------- Step 7 --------------------\n",
      "-------------------- Step 8 --------------------\n",
      "-------------------- Step 9 --------------------\n",
      "tf.Tensor([ 381   52 1079 ...  798  976  282], shape=(1024,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 1.9840572  -4.4717946   3.5500712  ... -4.620324    0.36508852\n",
      "  -1.8888571 ]\n",
      " [-0.6102125   1.8489007   2.438554   ... -1.7066348   1.6984487\n",
      "   2.954363  ]\n",
      " [ 1.          1.          1.         ...  1.          1.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.33503044  0.4592937  -1.4282337  ... -1.2023779  -1.5511016\n",
      "   1.8839315 ]\n",
      " [ 1.          1.          1.         ...  1.          1.\n",
      "   1.        ]\n",
      " [ 5.262765    1.6417694  -1.3948346  ... -4.4676905   3.3344588\n",
      "  -5.0418363 ]], shape=(1024, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def inference_with_saved_model(args):\n",
    "    hps.Init(global_batch_size = args[\"global_batch_size\"],\n",
    "             ps_config_file = args[\"ps_config_file\"])\n",
    "    model = tf.keras.models.load_model(args[\"saved_path\"])\n",
    "    model.summary()\n",
    "    def _infer_step(tmp_inputs, labels):\n",
    "        logit, embeddings = model(tmp_inputs)\n",
    "        return logit, embeddings\n",
    "\n",
    "    embeddings_peek = list()\n",
    "    inputs_peek = list()\n",
    "\n",
    "    sparse_keys, dense_features, labels = generate_random_samples(args[\"global_batch_size\"]  * args[\"iter_num\"], args[\"max_vocabulary_sizes\"], args[\"max_nnz\"][0], args[\"dense_dim\"])\n",
    "    dataset = tf_dataset(sparse_keys, dense_features, labels, args[\"global_batch_size\"])\n",
    "    for i, input_pack in enumerate(dataset):\n",
    "        inputs = []\n",
    "        for table_id in range(args[\"slot_num\"]):\n",
    "            inputs.append(tf.sparse.reshape(input_pack[table_id], [-1, input_pack[table_id].shape[-1]]))\n",
    "        inputs.append(input_pack[-2])\n",
    "        labels = input_pack[-1]\n",
    "        logit,embeddings = _infer_step(inputs, labels)\n",
    "        embeddings_peek.append(embeddings)\n",
    "        inputs_peek.append(inputs)\n",
    "        print(\"-\"*20, \"Step {}\".format(i),  \"-\"*20)\n",
    "\n",
    "    return embeddings_peek, inputs_peek\n",
    "\n",
    "embeddings_peek, inputs_peek = inference_with_saved_model(args)\n",
    "\n",
    "# embedding table, input keys are SparseTensor\n",
    "print(inputs_peek[-1][0].values)\n",
    "print(embeddings_peek[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
