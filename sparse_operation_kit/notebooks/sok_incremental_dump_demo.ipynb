{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4337359",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-sok-to-dlrm-demo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# SOK Incremental dump Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac179f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Due to the massive memory occupation of some users' Embedding tables, the cost of dumping the entire embedding table to the filesystem is significant. Therefore, SOK offers the option to input a certain time threshold and dump the key-value pairs updated after this time threshold into host memory. This notebook demonstrates how to use the SOK incremental_dump API.\n",
    "\n",
    "For more details about SOK, please refer to [SOK Documentation](https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202109ad",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get SOK from NGC\n",
    "\n",
    "SOK Python modules are preinstalled in the 23.12 and later [Merlin Tensorflow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:nightly`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import sparse_operation_kit as sok\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c936f7",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "SOK will dump the incrementally trained key-value pairs into a numpy array, allowing users to push these keys and values to their inference processes. This demo will show how to use the sok.incremental_dump API.\n",
    "\n",
    "The operation method of this Notebook is based on [Introduction to Horovod](https://enccs.github.io/upscalingAIcontainer/hvd_intro/?highlight=jupyter#training-with-model-fit). The process is to first define the function, and then use horovod.run to execute it.\n",
    "\n",
    "In the first of all, we need to configure some SOK Variable properties and define a function for SOK forward and backward. This function takes SOK variables, lookup ids, and an optimizer to perform the tasks of forward, backward, and optimizer update for SOK variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606a377a-fa7b-4345-91f9-f0cac53e81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "import numpy as np\n",
    "import sparse_operation_kit as sok\n",
    "\n",
    "rows = [8192 * 10, 8192]\n",
    "cols = [128, 4]\n",
    "hotness = [10, 3]\n",
    "combiners = [\"sum\", \"sum\"]\n",
    "batch_size = 8192\n",
    "iters = 5\n",
    "initial_vals = [13, 17]\n",
    "sleep_seconds = 15    \n",
    "\n",
    "# train step\n",
    "def train_step(params, indices,sok_optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        embeddings = sok.lookup_sparse(params, indices, combiners=combiners)\n",
    "        loss = 0\n",
    "        for i in range(len(embeddings)):\n",
    "            loss = loss + tf.reduce_sum(embeddings[i])\n",
    "    grads = tape.gradient(loss, params)\n",
    "    sok_optimizer.apply_gradients(zip(grads, params))\n",
    "    loss = hvd.allreduce(loss, op=hvd.Sum)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27de56-efc7-47db-bdc6-ebc23a17f89d",
   "metadata": {},
   "source": [
    "## Train the SOK variable for N iterations and incrementally dump the keys and values that are incrementally trained at each iteration\n",
    "\n",
    "In this section, starting from the second iteration, the code will incrementally dump the keys and values looked up in the previous iteration, and then compare them with the previously retained lookup keys to see if it is possible to incrementally dump all keys updated after a certain point in time. Note: incremental dump can only be performed in sok.DynamicVariable with HKV as the backend, and the internal time threshold of HKV is UTC time, therefore a UTC time threshold needs to be inputted.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039e7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sok_incremental_dump_evaluate():\n",
    "    sok_vars = [\n",
    "        sok.DynamicVariable(\n",
    "            dimension=cols[i],\n",
    "            var_type=\"hybrid\",\n",
    "            initializer=str(initial_vals[i]),\n",
    "            init_capacity=1024 * 1024,\n",
    "            max_capacity=1024 * 1024,\n",
    "        )\n",
    "        for i in range(len(cols))\n",
    "    ]\n",
    "    \n",
    "    optimizer = tf.optimizers.SGD(learning_rate=1.0, momentum=0.9)\n",
    "    sok_optimizer = sok.OptimizerWrapper(optimizer) \n",
    "    local_indices = []\n",
    "    for row in rows:\n",
    "        local_size = row // hvd.size()\n",
    "        if hvd.rank() < row % hvd.size():\n",
    "            local_size += 1\n",
    "        indices = np.arange(local_size) * hvd.size() + hvd.rank()\n",
    "        indices = tf.convert_to_tensor(indices, dtype=tf.int64)\n",
    "        local_indices.append(indices)\n",
    "                                                                                                                                                                                                                                                                                        # indices\n",
    "    total_indices = []\n",
    "    for i in range(len(rows)):\n",
    "        offsets = np.random.randint(1, hotness[i] + 1, iters * batch_size)\n",
    "        offsets = tf.convert_to_tensor(offsets, dtype=tf.int64)\n",
    "        offsets = hvd.broadcast(offsets, root_rank=0)\n",
    "        values = np.random.randint(0, rows[i], tf.reduce_sum(offsets))\n",
    "        values = tf.convert_to_tensor(values, dtype=tf.int64)\n",
    "        values = hvd.broadcast(values, root_rank=0)\n",
    "        total_indices.append(tf.RaggedTensor.from_row_lengths(values, offsets))\n",
    "        \n",
    "    left = batch_size // hvd.size() * hvd.rank()\n",
    "    right = batch_size // hvd.size() * (hvd.rank() + 1)  \n",
    "    \n",
    "    indices_records = []\n",
    "    time_records = []\n",
    "    for i in range(iters):\n",
    "        indices = []\n",
    "        indices_global = []\n",
    "        for j in range(len(total_indices)):\n",
    "            indices.append(total_indices[j][i * batch_size + left : i * batch_size + right])\n",
    "            indices_global.append(total_indices[j][i * batch_size : (i + 1) * batch_size])\n",
    "        time.sleep(sleep_seconds)\n",
    "        loss = train_step(sok_vars, indices,sok_optimizer)\n",
    "        indices_records.append(indices_global)\n",
    "        time.sleep(sleep_seconds)\n",
    "    \n",
    "        # Must convert now time threshold to UTC time threshold\n",
    "        utc_time = datetime.now(pytz.utc)\n",
    "        time_records.append(utc_time)\n",
    "        if i > 0:\n",
    "            time_before = time_records[i - 1]\n",
    "            #incremental_model_dump will accept multi sok_vars , and return multi keys , values\n",
    "            #len(sok_vars) == len(keys) == len(values)## Run With Horovod\n",
    "            keys, values = sok.incremental_model_dump(sok_vars, time_before)\n",
    "        \n",
    "            num_lookups = len(keys)\n",
    "            indices_before = indices_records[i]\n",
    "            for lookup_id in range(num_lookups):\n",
    "                indices_flat_before = indices_before[lookup_id].flat_values\n",
    "                indices_np = indices_flat_before.numpy()\n",
    "                indices_np, unique_reverse_indices = np.unique(indices_np, return_index=True)\n",
    "                indices_np = np.sort(indices_np)\n",
    "                tmp_keys = keys[lookup_id]\n",
    "                tmp_keys = np.sort(tmp_keys)\n",
    "                np.testing.assert_array_equal(indices_np, tmp_keys)\n",
    "            print(\"____________iter {} is pass!________________\".format(str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307fec3",
   "metadata": {},
   "source": [
    "Define a func to call sok_incremental_dump_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18499433-4b69-4d39-9337-832b754af8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import horovod\n",
    "\n",
    "def training_func():\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    import time\n",
    "    import pytz\n",
    "    from datetime import datetime\n",
    "    import tensorflow as tf\n",
    "    import horovod.tensorflow as hvd\n",
    "    import numpy as np\n",
    "    import sparse_operation_kit as sok\n",
    "\n",
    "\n",
    "    hvd.init()\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "    sok.init()\n",
    "\n",
    "    #remove some tf stderr output\n",
    "    class suppress_stderr:\n",
    "        def __init__(self):\n",
    "            self.null_fd = os.open(os.devnull, os.O_RDWR)\n",
    "            self.save_fd = os.dup(2)\n",
    "\n",
    "        def __enter__(self):\n",
    "            os.dup2(self.null_fd, 2)\n",
    "\n",
    "        def __exit__(self, *_):\n",
    "            os.dup2(self.save_fd, 2)\n",
    "            os.close(self.null_fd)\n",
    "            os.close(self.save_fd)\n",
    "        \n",
    "    with suppress_stderr():\n",
    "        sok_incremental_dump_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74fa65",
   "metadata": {},
   "source": [
    "## Run With Horovod\n",
    "\n",
    "use horovod.run do 2 process task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f685fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1,0]<stderr>:2024-06-03 01:52:59.067307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "[1,0]<stderr>:2024-06-03 01:52:59.067348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "[1,0]<stderr>:2024-06-03 01:52:59.068589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[1,1]<stderr>:2024-06-03 01:52:59.104917: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "[1,1]<stderr>:2024-06-03 01:52:59.104956: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "[1,1]<stderr>:2024-06-03 01:52:59.106187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,0]<stdout>:[SOK INFO] Import /usr/local/lib/libsparse_operation_kit.so\n",
      "[1,1]<stdout>:[SOK INFO] Import /usr/local/lib/libsparse_operation_kit.so\n",
      "[1,0]<stdout>:[SOK INFO] Initialize finished, communication tool: horovod\n",
      "[1,1]<stdout>:[SOK INFO] Initialize finished, communication tool: horovod\n",
      "[1,0]<stdout>:____________iter 1 is pass!________________\n",
      "[1,1]<stdout>:____________iter 1 is pass!________________\n",
      "[1,0]<stdout>:____________iter 2 is pass!________________\n",
      "[1,1]<stdout>:____________iter 2 is pass!________________\n",
      "[1,0]<stdout>:____________iter 3 is pass!________________\n",
      "[1,1]<stdout>:____________iter 3 is pass!________________\n",
      "[1,0]<stdout>:____________iter 4 is pass!________________\n",
      "[1,1]<stdout>:____________iter 4 is pass!________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horovod.run(training_func, np=2, verbose=False, disable_cache=True, use_mpi=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
